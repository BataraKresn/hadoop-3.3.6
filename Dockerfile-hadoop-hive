# Use Ubuntu 18.04 as the base image
FROM ubuntu:18.04 AS base

# Set environment variables
ENV HADOOP_VERSION=3.3.6
ENV HADOOP_CONF_DIR=/etc/hadoop
ENV MULTIHOMED_NETWORK=1
ENV HADOOP_HOME=/opt/hadoop-$HADOOP_VERSION
ENV SPARK_VERSION=spark-3.5.1
ENV SPARK_HOME=/opt/$SPARK_VERSION
ENV PYSPARK_PYTHON=python3.8
ENV PYTHONHASHSEED=1
ENV HIVE_VERSION=3.1.3
ENV HIVE_HOME=/opt/hive-$HIVE_VERSION
ENV PATH=$HADOOP_HOME/bin/:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$HIVE_HOME/bin:$PATH

# Update package list and install required dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    openjdk-11-jdk \
    openssh-client \
    nano \
    software-properties-common \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Add deadsnakes PPA and install Python 3.8
RUN add-apt-repository -y ppa:deadsnakes/ppa \
    && apt-get update && apt-get install -y --no-install-recommends \
    python3.8 \
    python3.8-distutils \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Set Python 3.8 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1

# Install pip for Python 3.8
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    && rm -rf /var/lib/apt/lists/* \
    && wget https://bootstrap.pypa.io/get-pip.py \
    && python3.8 get-pip.py \
    && rm get-pip.py

# Install additional dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    gnupg2 \
    net-tools \
    build-essential \
    zlib1g-dev \
    libncurses5-dev \
    libgdbm-dev \
    libnss3-dev \
    libssl-dev \
    libreadline-dev \
    libffi-dev \
    gcc \
    g++ \
    subversion \
    gfortran \
    redis-tools \
    nano \
    libopenblas-dev \
    liblapack-dev \
    libqpdf-dev \
    pkg-config \
    libzbar-dev \
    qpdf \
    xvfb \
    gconf-service \
    libasound2 \
    libatk1.0-0 \
    libcairo2 \
    libcups2 \
    libfontconfig1 \
    libgdk-pixbuf2.0-0 \
    libgtk-3-0 \
    libnspr4 \
    libpango-1.0-0 \
    libxss1 \
    fonts-liberation \
    libappindicator1 \
    libnss3 \
    lsb-release \
    xdg-utils \
    netcat-openbsd \
    dnsutils \
    iputils-ping \
    tcpdump \
    net-tools \
    telnet \
    && apt-get autoremove -yqq --purge \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install pip and Python packages
RUN python3.8 -m pip install --default-timeout=100 --upgrade pip
RUN python3.8 -m pip install pikepdf Cython numpy wheel setuptools --force-reinstall

# Copy Hadoop and Spark tar files and extract them
COPY hadoop-3.3.6.tar.gz /tmp/
COPY spark-3.5.1-bin-hadoop3.tgz /tmp/

RUN set -x \
    && tar -xvf /tmp/hadoop-3.3.6.tar.gz -C /opt/ \
    && rm /tmp/hadoop-3.3.6.tar.gz \
    && ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop \
    && mkdir /opt/hadoop-$HADOOP_VERSION/logs \
    && mkdir /hadoop-data \
    && tar -xvzf /tmp/spark-3.5.1-bin-hadoop3.tgz -C /opt/ \
    && rm /tmp/spark-3.5.1-bin-hadoop3.tgz \
    && mv /opt/${SPARK_VERSION}-bin-hadoop3 /opt/${SPARK_VERSION}

# Download and install Hive
RUN wget https://downloads.apache.org/hive/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz -P /tmp/ \
    && tar -xvzf /tmp/apache-hive-${HIVE_VERSION}-bin.tar.gz -C /opt/ \
    && rm /tmp/apache-hive-${HIVE_VERSION}-bin.tar.gz

# Copy configuration files
COPY conf/core-site.xml $HADOOP_CONF_DIR/core-site.xml
COPY conf/hdfs-site.xml $HADOOP_CONF_DIR/hdfs-site.xml
COPY conf/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
COPY conf/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml
COPY hive/hive-base/conf/hive-site.xml $HIVE_HOME/conf/hive-site.xml
COPY conf/core-site.xml $SPARK_HOME/conf
COPY conf/yarn-site.xml $SPARK_HOME/conf

# Copy and set entrypoint script
COPY entrypoint.sh /entrypoint.sh
RUN chmod a+x /entrypoint.sh

# Copy Python requirements file and install packages
COPY requirements.txt /requirements.txt
RUN python3.8 -m pip install --default-timeout=600 -r /requirements.txt

# Set entrypoint
ENTRYPOINT ["/entrypoint.sh"]
